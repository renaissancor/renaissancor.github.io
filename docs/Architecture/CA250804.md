# Cache Memory 

The basic transfer unit of a CPU cache is called a cache line, 
and its size is typically 64 bytes.
This takes advantage of spatial locality, 
as the cache loads an entire 64-byte block from RAM at once.

Go to 작업 관리자 / 성능 / CPU 
Check L1 Cache Status. For instance, 

- L1  256 KB 
- L2 1024 KB = 1 MB 
- L3 6000 KB = 6 MB 

Hyperthreading Core / Logic Processor (Thread) 
One Core is displayed like two Cores, but not actually two. 

Cache Line is Unit of 64 Byte Size.  

**Example:**
L1 Cache = 256 KB  
Cache Line Size = 64 Bytes  

Number of Cache Lines =  
\[
\frac{256 \times 1024}{64} = 4096 \text{ lines}
\]

L1 Cache Lines Saved 
L1 Cache in this CPU can be perceived as having 4096 Cache Lines. 
Or, it can be considered like `CacheLines[4096]` in total. 

Then, HOW to bring data from Cache Memory? 
Brute-forcing through all 4096 cache lines? Of course not. 
That would be highly inefficient.

Hash Function is not bad try, still there exists better approach. 
The lower bits of the memory address are divided into the offset 
(to locate the exact byte within a cache line) and the index 
(to determine the specific cache set). 
The remaining higher bits are used as the tag.

Why last parts? because first parts are generally same values. 
Generally Stack memory have about few bytes difference, meaning that 
their lower bits are different while higher bits generally never change. 
```
|00 00 00 00 00 00 00|00 00 00 00 00 00 00 00 00  
\____________________/
  12 Bits Index & Offset 
```
If index and offset become higher bits, 
it means that lower 20 bits should be changed to change upper bit Offset, 
and 26 bits should be changed to change index. 
$2^{20} = 2097152 = 2$ MB, $ 2^{26} = 134217728 = 128$ MB. 
Thus, using upper bits means Cache will be in no overlap when about 
memory access jumping about 128MB in default should happen, meaning 
it is very unlikely. 

Using the lower 12 bits for offset and index is completely valid. Given that 
access to some memory regions happen very frequently, inside about 64 KB in 
this CPU, the Cache Hit will happen theoretically always (although OS context 
switching will not guarantee 100% Cache Hit), maximizing performance of CPU Cache Memory. 

## Cache Memory Calculation 

Calculate How Computer Cache Memory operates, and able to calculate it.  

Download tools like `CPUZ` to check detailed CPU Cache information. 
(Download CPU-Z in CPUID)[https://www.cpuid.com/softwares/cpu-z.html] 

For instance, if L1 Cache is 256KB and physical Core is 4, then 
L1 Cache will be distributed 64KB per each core. 
It is not always 1 : 1 ratio, but in this example suppose that 
Instruction Cache is 32 KB and Data Cache iw 32 KB. 

Since Cache Line Size is 64 Bytes, 32KB Cache can save 32 X 1024 / 64 = 512 Cache Lines. 
This Cache structure can also be described as structure of 
Cache_Lines[512]; With 512 Index numbers to access Cache Lines. 
Then, how would Cache find out existence of address? 
Brute Force from 0 to all 512 existing Cache Lines is very inefficient 
and not likely. 

Suppose 32 Bit Operating System, with 32 Bit Virtual Address. 

```txt 
|00 00 00 00 00 00 00 00 00 00|00 00 00|00 00 00| 
 \___________________________/ \______/ \______/
            20 bit              6 bit     6 bit 
               tag              index     offset    
```

64 Bit OS will be just more bytes in front. 
Technically 32 Bits more tag, but actually about 16 More bits since 
address of 512Tb is still far more than enough and 32 more bits is 
tremeondously huge numbers. 

```
[ 32-bit Address ]
|  Tag (20b) | Index (6b) | Offset (6b) |
|___________|____________|_____________|
            |            \-> Which Set?
            \-> Which Memory Block?
```

Since Cache Line is 64 Bytes, 6 Bit Offset is required to represent 
all address from 0 to $2^6 = 64$ possible detailed addresses inside 
one Cache Line. 

### Cache Mapping 

Cache Collision and N-Way Set Assosiative Cache 

**Direct Mapped Cache** 

One Dimensional Array of Cache sets 
Each Cache set include Cache Line, Tag value, and valid bit 
1 : 1 Mapping, one cache line assigned to one memory block 

However, any single cache collision will reset cache. 
In function using stack memory only it might be useful, 
but modern programming that access heap, .data .rodata .bss etc ... 
will result in countless cache miss, as they access 
different virtual addresses far away from each other. 

**Fully Associative Cache** 

Letting every cache lines, or 64 bit blocks can be inside Cache Memory, 
and perform search for all cache hit check. 
This is unrealistic in moder CPU. If really trying to implement this kind of
computer organization, very complicated algorithm to check cache hit with 
complicated circuit will be required, which is quite unlikely. 

**Set Associative Cache** 

Since fully associative Cache is unlikely to implement algorithm and circuit, 
and direct memory access is also inefficient due to the modern programming language 
that access stack, heap, data, etc creating several cache collision, 
more alternative approach is required nowadays. 

If Cache with Same Offset can be saved multiple times, like 8 times for each 
same offset, so that one Index can save multiple value of addresses, then 
Each Index of L1 Cache Memory will save maximum N times (8 in here) of different 
addresses with same Index variable. 

```
[0] ---- Index [0] Cache Line 0X1F2D3000 // Stack  
[1]  \-- Index [1] Cache Line 0X3F4D3000 // Heap
[2]   \- Index [2] Cache Line 0X0F123000 // Data 
...      ...   [N] Whatever 
[63]
```

This kind of multiple Cache Line save per each Cache Index might be possible. 
Then, instead of Direct Memory Access with about 512 KB per each Core, 
This Set Associative Cache will have $512 / 8 = 64$ Sets. 
This means 64 index, so less bits per each index. 
Instead, same index several Memory blocks will be saved without collision.  
In this case, 8 is `ways`, so `L1` will be 64KB 8 Ways Cache.  

It means smaller size of cache memory space in linear perspective, 
but maximum 8 allowed cache memory save with same offset and index will be allowed. 

Considering that this same index and offset is very likely when accessing 

## Paging and Cache Memory 

However, even considering usage of several Memory regions like Heap and .data, 
it seems too much to have about 8 ways with 64 KB Cache only per each core, 
even though bigger Cache capacity will increase performance. 

Memory conversion from virtual to physical is through MMU. 
TLB (Translation Lookaside Buffer) is a cache to contain mapping information 
between virtual and physical address inside MMU. 

Then how does Cache Memory Saved? Physical Address or Virtual Address? 
Virtual Address Based Mapping do NOT require address conversion, but 
the problem is that several processes might use same virtual address. 

Physical Address Based Mapping guarantees uniquity of address, but it enforces 
virtual address to physical address conversion for every cache memory data access. 

Modern CPU combines both physical address and virtual address. 
VIPT(Virtually-Indexed, Physically-Tagged).  
In detail, virtual address is used to find cache index and offset, 
while cache hit is checked by physical address tag check.  

### Page Offset 

Virtual Memory and Physical Memory Mapping Unit is $2^{12} = 4096 = 4$ KB. 
This inner offset is same in both virtual and physical memory . 
Thus, if index and offset bit are in the page offset range, 
Cache location is detectable without virtual to physical address conversion. 
Then, use physical address uppper bits as `tags` to check it is target data or not. 

Paging Unit from Virtual Memory to Physical Memory in RAM is always 
different and unpredictable. 

Actually, there is one more crucial reason why modern CPU currently get stuck 
in 64KB Cache Memory size which seems not big in several ways. 
Virtual Memory and Physical Memory are connected by Paging method. 

Cache Memory, despite its size gets bigger, maintains maximum 64 Cache Lines per 
each L1 Cache, without increasing index from 64 to 128 or 256. 
This is mainly due to the Page Offset 4KB, 12 Bit. 

So, what if Index, aka Set, increases with same Page Offset 4 KB 12 Bit? 
If index and offset are handled Inside 12 Bit, data processing is possible 
without MMU. 

However, if index became 7 bits, total index + offset gets bigger than page offset, 
so it could consider DIFFERENT physical Frame Addresses mapped from Virtual 
Page Addresses would be unpredictable, existing TWO Frames. 
Thus, either MMU pass would be necessary, or virtual memory based 
index calculation will result in unpredictable behavior between two 
different Physical RAM Frames mapped from one virtual page. 

In the future, it will happen simultaneously that 

- Page Offset 4 KB to 8 KB 
- Cache Line Maximum Index 64 to 128 


### Vocabulary 

Cache Line 

- Cache Memory Basic Transfer Unit of 64 Bytes 
- One Memory access bring 64 Bytes Unit, Spacial Locality 

Frame 

- Physical Memory Unit Maximum  4 KB, 4096 Bytes 
- Mapped to Virtual Memory 

Page Offset 

- Bytes location inside Page 
- Virtual Memory Page Unit Maximum 4 KB, 4096 Bytes 
- Virtual Memory and Physical Memory Low 12 Bits are Same 

Index (Set) 

- Cache Data input expected Cache Set 
- $2^{index_bits}$ number of Sets(index) exists 
- Each set has one Cache Line in Direct Mapped Cache 
- Each set has several Cache Lines in N-Way Set Assosiative Cache 

Tag 

- Check Data saved in Cache is what block 
- Use upper bit address index 
- Except Index and OFfset 

Way (N-Way) 

- Cache Lines per each Set (Index) 
- For instance, 4-Way, one index set 8 cache lines savable 
- Used in Set Associative Cache 

TLB (Translation Lookaside Buffer) 

- Address Virtual to Physical Conversion Cache in MMU 
- Quickly switch virtual address to frequently used Physical Address 

Locality 
- Temporal Locality : Recently used data likely to be reused 
- Spatial  Locality : Access to address likely to cache hit recent address, Cache Line 

