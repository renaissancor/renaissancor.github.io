# Process 

Priority Code whatever  
Detailed Info 

우선 순위 설정 / 실시간 높음 높은 우선 순위 보통 낮은 우선 순위 낮음 

선호도 설정 exe 를 실행할 수 있는 프로세서 CPU 0 ~ CPU ? 최대 개수  

STL is originally heavy. 
Several wrapped functions inside. 
However, these wrapping will be optimized in release mode Compile. 
If STL gets inline STL is fast too. 

Memory access near previous value means more Cache hit likelihood.  
Further memory means more likelihood of cache miss. 
L1 L2 L3 Caches also have miss potential. 
The L1 Cache miss and L2 Cache miss has about twice speed difference. 

Data Structure Stack size will also change performance 
Stack LIFO increase sequentially from down to up, so cache miss at the
first try is likely inevitable, but after than it gets 100% hit mostly. 
Queue has different input and output location FIFO so it has 
different input space and output stace so that cache miss is 
more likely than stack. 
CPU Preference, attach certain process / thread into certain CPU Core, 
so that external interruption is minimized during benchmark. 

Intel CPU all Core share L3 Cache  
AMD CPU has L3 Cache per each Core  

### Flood Fill 

Pathfinding 


DFS go to the straignt place so it is hard to find minimum distance by DFS only, 
but finding path itself might be quicker than BFS, but more efficient to find shortest path. 
Dijakstra includes weight for path finding to figure out more efficient direction, 
including weight per each node. 

A Star is BFS based 

### Cache 

One CPU includes multiple cores 
Bigger Cache memory means more expensive. 
Cache memory size is decided per each architecture. 

Intel 13th CPU began to have 
low electricity Core (Efficiency Core) and 
High performance Core. 

Latest Intel CPUS have sometimes over 20 cores, but it includes 
multiple efficiency cores. 

Threads created through programming also swaps 
between performance core and efficiency core. 

There exists function to let core use high performance, or 
give priority. 

Intel CPU Cores share L3 Cache. they take a look at L1, L2 and L3 
when all of them result in miss then it is finally Cache miss. 

First time read variable from RAM, then it will let L1 L2 L3 Cache 
to load the data from RAM to Cache, then CPU register will take 
data from Cache. 

Calculation and measurement of Cache Hit is very challenging, 
so realistically watching and measuring L1 Cache Hit only is enough at first. 

L1 Cache Hit and L3 Cache hit has about over 2 times performance speed difference. 
Thus, minimizing L1 Cache miss is the final goal. 

Cache Miss 100% probability is guaranteed, 
However Cache 100% Hit is unlikely due to the multithreading, but see the probability. 

Temporal Locality - Recently used data is more likely to cache hit 
Spacial Locality - Data near previously used data is more likely to cache hit 

Spacial locality 
Memory drag data then drag cache line too. 
Cache Line is 64 Byte. Even though only 1 byte is read still 
unit is 64 Bytes. When int a and int b exist and read a and b then 
if data line including a is dragged then b is also likely to be in cache. 

Every data should pass Cache to enter CPU register 

Some devices pass to the RAM without CPU 
But not multithreading C++ programming perspective. 

Non Cache Memory 

Some people misunderstand that `volatile` access by direct RAM without cache, 
but it is completely false. 



## `alignas(int)` function 

Waste of Cache Memory 

Cache Line Function 

alignas is not often used 
